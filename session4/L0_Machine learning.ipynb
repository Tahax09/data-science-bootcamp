{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proability is amazing - Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning from data\n",
    "\n",
    "Given a dataset and a model we have these these interesting probabilites \n",
    "\n",
    "- **The likelihood** $P(Data|Model)$\n",
    "- **Model fitting**: $P(Model|Data)$ \n",
    "- **Predictive distribution**: $P(New data| Model, Data)$ \n",
    "\n",
    "Togehter these retells the machine learing story in a probabilistic words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplest learning algorithm never told (the counting)! \n",
    "Suppose we have a dataset of coin flips $D$, and we want to learn $P(C=H)$, here is the simple algorithm\n",
    "- Count the heads\n",
    "- Divide by the total trails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "def generateData(true_p, dataset_size):\n",
    "    return ['H' if i == 1 else 'T' for i in st.bernoulli.rvs(0.3 ,size=dataset_size)]\n",
    "\n",
    "def estimate_p(data):\n",
    "    return sum([1.0 if observation == 'H' else 0.0 for observation in data]) / len(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true p: 0.3\n",
      "learned p: 0.30155\n"
     ]
    }
   ],
   "source": [
    "#simulate data\n",
    "true_p = 0.3\n",
    "dataset_size = 20000\n",
    "\n",
    "data = generateData(true_p, dataset_size)\n",
    "p_hat = estimate_p(data)\n",
    "\n",
    "#print (data)\n",
    "print (\"true p:\", true_p)\n",
    "print (\"learned p:\", p_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "Plot accurcy vs. dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The counting algorithm origin\n",
    "\n",
    "First we could model coins by bernulli random variable $c$ ~ $Ber(\\theta)$\n",
    "\n",
    "now the liklyhood of the data is $$likelihood = P(Data|\\theta) = \\prod{\\theta^x * (1 - \\theta)^{1 - x}}$$\n",
    "\n",
    "And we want $\\theta$ that makes the likelihood maximum $$\\theta_{mle} = argmax_{\\theta}\\prod{\\theta^x * (1 - \\theta)^{1 - x}}$$\n",
    "\n",
    "This maximum likelihood is called MLE or the maximum likelihood estimator, if you do the math (derive likelihood and find the zeros) then you will find that $$\\theta_{mle} = \\frac{\\sum(C=H)}{N}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***\"All models are wrong, but some models are useful. â€” George Box\"***\"\n",
    "\n",
    "For the next iterations, the baise may change, the coin may got curved, why we can relay on mle estimator ?\n",
    "The answer is subtile but very fundemental to machine learing, it is a theorem called **No Free launch theorem**. It states that we could not learn without making assumption.\n",
    "\n",
    "### Exercise: \n",
    "What is the assumption we made to make it happen ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does machine learning scale ? (Independencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import scipy.stats as st \n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "def generateData(true_p, dataset_size):\n",
    "    data= np.array([ ['H' if j == 1 else 'T' for j in st.bernoulli.rvs(true_p[i] ,size=dataset_size)] for i in range(0, len(true_p))])\n",
    "    return data.T.tolist()\n",
    "    \n",
    "\n",
    "def estimate_p(data):\n",
    "    d = len(data[0])\n",
    "    if d > 23:\n",
    "        raise 'too many dims'\n",
    "    toCross = [['H', 'T'] for i in range(0, d)]\n",
    "    omega = list(itertools.product(*toCross))\n",
    "    combos = {tuple(x): 0 for x in omega}\n",
    "    \n",
    "    for i in data:\n",
    "        combos[tuple(i)] += 1\n",
    "    n = len(data)\n",
    "    p = {k: float(v)/n for (k,v) in combos.items()}\n",
    "    return p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = 12\n",
    "pp = 0.25\n",
    "data = generateData([pp for i in range(0, d)], 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true p:  5.960464477539063e-08\n",
      "0.0\n",
      "1 loops, best of 1: 7.82 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "p = estimate_p(data)\n",
    "\n",
    "print (\"true p: \" , pp**d)\n",
    "print (p[tuple(['H' for i in range(0, d)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def estimate_p_ind(data):\n",
    "    d = len(data[0])\n",
    "    omega = [(i, 'H') for i in range(0, d)] + [(i, 'T') for i in range(0, d)]\n",
    "    combos = {x: 0 for x in omega}\n",
    "    \n",
    "    for i in data:\n",
    "        for ix, j in enumerate(i):\n",
    "            combos[(ix, j)] += 1\n",
    "    n = len(data)\n",
    "    p = {k: float(v)/n for (k,v) in combos.items()}\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true p:  5.960464477539063e-08\n",
      "5.9374260560847265e-08\n",
      "1 loops, best of 1: 30.4 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "p_ind = estimate_p_ind(data)\n",
    "#p('H', 'H', 'H', 'H') = prod P('H')\n",
    "t = [p_ind[(i, 'H')] for i in range(0, d)]\n",
    "p_Hs = functools.reduce(lambda x, y: x* y, t, 1)\n",
    "\n",
    "print (\"true p: \" , pp**d)\n",
    "print(p_Hs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability is a database\n",
    "#### The story so far \n",
    "\n",
    "- Probability is a database\n",
    "- Model is schema allow us to learn\n",
    "- We observe and insert (fit) a trainng set on bulk using optimization\n",
    "- we predict using inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
